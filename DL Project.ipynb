{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "plastic-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State setting of the board and players\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3\n",
    "num_actions = 9\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, player1, player2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # player player 1 goes first by initializing it first\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    # getting the unique hash of the current state of the board\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return self.boardHash\n",
    "\n",
    "    # logic to get the winner\n",
    "    def winner(self):\n",
    "\n",
    "        # checking row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "\n",
    "        # checking column\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "\n",
    "        # checking diagonal\n",
    "        diag_sum1_l_to_r = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2_r_to_l = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(abs(diag_sum1_l_to_r), abs(diag_sum2_r_to_l))\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            if diag_sum1_l_to_r == 3 or diag_sum2_r_to_l == 3:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        # checking for tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "\n",
    "    # checking for all the redundant positions\n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                # 0 indicates not occupied and is available\n",
    "                # thus, if 0 the we append that board position into positions array\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "\n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "\n",
    "    # The game end !!\n",
    "    def giveReward(self):\n",
    "\n",
    "        result = self.winner()\n",
    "\n",
    "        # backpropagate reward\n",
    "        # player 1 is winner\n",
    "        if result == 1:\n",
    "            self.player1.feed_Reward(1)\n",
    "            self.player2.feed_Reward(0)\n",
    "\n",
    "        # player 2 is winner\n",
    "        elif result == -1:\n",
    "            self.player1.feed_Reward(0)\n",
    "            self.player2.feed_Reward(1)\n",
    "\n",
    "        # game is a tie\n",
    "        else:\n",
    "            self.player1.feed_Reward(0.1)\n",
    "            self.player2.feed_Reward(0.5)\n",
    "\n",
    "    # Resetting the board\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    def train(self):\n",
    "        epoch = 100\n",
    "        for episode in range(epoch):\n",
    "            print(\"Epoch {} / {}\".format(episode, epoch))\n",
    "            actions = []\n",
    "            while not self.isEnd:\n",
    "                if self.playerSymbol > 0:\n",
    "                    action = self.player1.make_Move(self.availablePositions(), self.board, self.playerSymbol)\n",
    "                    actions.append(action)\n",
    "                    self.player1.add_State(self.board)\n",
    "                    current_state = self.board\n",
    "                    if action not in self.availablePositions():\n",
    "                        self.player1.feed_Reward(-2)\n",
    "                    else:\n",
    "                        self.updateState(action)\n",
    "                    next_state = self.board\n",
    "                else:\n",
    "                    action = self.player2.make_Move(self.availablePositions(), self.board, self.playerSymbol)\n",
    "                    actions.append(action)\n",
    "                    self.player2.add_State(self.board)\n",
    "                    current_state = self.board\n",
    "                    if action not in self.availablePositions():\n",
    "                        self.player2.feed_Reward(-2)\n",
    "                    else:\n",
    "                        self.updateState(action)\n",
    "                    next_state = self.board\n",
    "                if len(self.availablePositions()) ==0:\n",
    "                    self.isEnd = True\n",
    "            self.giveReward()\n",
    "            self.reset()\n",
    "            self.player1.train_model()\n",
    "            self.player2.train_model()\n",
    "            self.player1.reset()\n",
    "            self.player2.reset()\n",
    "        return self.player1, self.player2\n",
    "\n",
    "    # playing with human player\n",
    "    def playWithHuman(self):\n",
    "        while not self.isEnd:\n",
    "            # Player_1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            # self.showBoard() --------------------------------Fernando needs to do GUI for this portion\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"It's a win!\")\n",
    "                else:\n",
    "                    print(\"It's a tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player_2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                # self.showBoard() --------------------------------Fernando needs to do GUI for this portion\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"It's a win!\")\n",
    "                    else:\n",
    "                        print(\"It's a tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, name, exp_rate=0.3):\n",
    "        self.name = name\n",
    "        self.states = []  # array of every position that was taken\n",
    "        self.lr = 0.2\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # states should correspond to the value\n",
    "        self.actions = []\n",
    "        self.model = self.create_model()\n",
    "        # \"hack\" implemented by DeepMind to improve convergence\n",
    "        # self.target_model = self.create_model()\n",
    "        self.rewards = []\n",
    "\n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return boardHash\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        \"\"\"\n",
    "        :param filename: name of the model\n",
    "        sets the self.model and self.target_model to the model passed in\n",
    "        :return: none\n",
    "        \"\"\"\n",
    "        self.model = load_model(filename)\n",
    "        # self.target_model = load_model(filename)\n",
    "\n",
    "    # Saves model to specified path\n",
    "    def save_model(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        MODIFY THIS\n",
    "        Defines and creates the neural network\n",
    "        :return: returns the neural network model\n",
    "        \"\"\"\n",
    "        # FIX MODEL\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(12, (2, 2), activation='relu', input_shape=(3, 3, 1)))\n",
    "        self.model.add(layers.BatchNormalization())\n",
    "        self.model.add(layers.Flatten())\n",
    "        self.model.add(Dense(num_actions))\n",
    "        self.model.compile(loss=\"BinaryCrossentropy\",\n",
    "                           optimizer=Adam(learning_rate=self.lr))\n",
    "        self.model.summary()\n",
    "        return self.model\n",
    "\n",
    "    def make_Move(self, positions, current_board, symbol):\n",
    "        \"\"\"\n",
    "        :param positions:\n",
    "        :param current_board:\n",
    "        :param symbol:\n",
    "        :return: index position for where the player will place their piece next\n",
    "        \"\"\"\n",
    "        action = 0\n",
    "        next_board = current_board.copy()\n",
    "        if np.random.uniform(0, 1) <= self.epsilon:\n",
    "            #  make a random move on the board\n",
    "            index = np.random.choice(len(positions))\n",
    "            action = positions[index]\n",
    "        else:\n",
    "            next_board = np.ndarray.flatten(current_board).tolist()\n",
    "            action = self.model.predict(current_board)\n",
    "        # Reduce the number of random actions as the model learns more\n",
    "        self.epsilon = self.epsilon * self.epsilon_decay if self.epsilon >= self.epsilon_min else self.epsilon_min\n",
    "        self.add_State(current_board)\n",
    "        self.actions.append(action)\n",
    "        return action\n",
    "\n",
    "    def train_model(self):\n",
    "        # state_list = [np.ndarray.flatten(x) for x in self.states]\n",
    "        # state_list = [x.tolist() for x in state_list]\n",
    "        state_list = [np.expand_dims(state, axis=0) for state in self.states]\n",
    "        state_list = [np.expand_dims(state, axis=-1) for state in state_list]\n",
    "        # state_list = np.asarray(state_list)\n",
    "        # state_list = state_list.reshape(len(state_list), 3, 3, 1)\n",
    "        self.model.fit(state_list, self.rewards)\n",
    "\n",
    "    # additional hashstate\n",
    "    def add_State(self, state):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(0)\n",
    "\n",
    "    def get_state(self, index=-1):\n",
    "        return self.states[index]\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feed_Reward(self, reward):\n",
    "        for st in range(len(self.states)):\n",
    "            self.rewards[-st] += self.lr * (self.decay_gamma * reward - self.rewards[-st])\n",
    "            reward = self.rewards[-st]\n",
    "            # self.rewards.append(reward)\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "    def save_Policy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def load_Policy(self, file):\n",
    "        fr = open(file, 'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()\n",
    "\n",
    "\n",
    "class HumanPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def make_Move(self, positions):  # Have the player insert a row and column\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "\n",
    "    # append a hash state\n",
    "    def add_State(self, state):\n",
    "        pass\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feed_Reward(self, reward):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # train the NN\n",
    "\n",
    "    P1 = Player(\"P1\")\n",
    "\n",
    "    P2 = Player(\"P2\")\n",
    "\n",
    "    cs = State(P1, P2)\n",
    "\n",
    "    print(\"Training the Neural Network...\")\n",
    "\n",
    "    play1, play2 = cs.train()\n",
    "    play1.save_model(\"player1\")\n",
    "    play2.save_model(\"player2\")\n",
    "\n",
    "    # Play against a human\n",
    "    P1 = Player(\"Computer Player\", exp_rate=0)\n",
    "\n",
    "    P1.load_model(\"policy_P1\")\n",
    "\n",
    "    P2 = HumanPlayer(\"Human\")\n",
    "\n",
    "    cs = State(P1, P2)\n",
    "    cs.playWithHuman()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-photography",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
